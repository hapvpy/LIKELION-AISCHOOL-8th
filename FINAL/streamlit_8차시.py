# ì‚¬ì§„ ì˜† ì¬ë£Œë²„íŠ¼ -> ë§í¬ì—°ê²° (ì±—ë´‡í˜•ì‹0)


## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st 

import requests
from bs4 import BeautifulSoup

import pandas as pd
import numpy as np
import copy
import json

from ast import literal_eval

import torch
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModel
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from transformers import pipeline
from transformers import GPT2TokenizerFast
from PIL import Image
from tqdm import tqdm

import pickle
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict

import sklearn.datasets as datasets
import sklearn.manifold as manifold

import openai
import os
import io
import sys
from dotenv import load_dotenv
from streamlit_chat import message



## íŒŒì¼ ë° API ê°€ì ¸ì˜¤ê¸°
# app.py íŒŒì¼ì´ ìœ„ì¹˜í•œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
APP_DIR = os.path.dirname(os.path.abspath(__file__))
# app.pyì—ì„œ ë§Œë“  ë°ì´í„°í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°
LAST_DF_PATH = os.path.join(APP_DIR, '..', 'last_df.pkl')
df = pd.read_pickle(LAST_DF_PATH)
df['feature'] = df['ì¬ë£Œ']
openai.api_key = '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'

cur_os = sys.platform



## ëª¨ë¸ ì„ ì–¸
# mps_device = torch.device('mps')
model = SentenceTransformer('BM-K/KoSimCSE-roberta-multitask')



## ìƒìœ„ 5ê°œ í•­ëª© ì¶œë ¥(ë§í¬ë¡œ ì¤‘ë³µì œê±°-ë¯¼ê·œë‹˜)
def get_query_sim_top_k(query, model, df):
    "ì¿¼ë¦¬ì™€ ë°ì´í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê³  ìœ ì‚¬í•œ ìˆœìœ„ 5ê°œ ë°˜í™˜"
    query_encode = model.encode(query)
    cos_scores = util.pytorch_cos_sim(query_encode, df['ko-sroberta-multitask-feature6'])[0]
    top_results = torch.topk(cos_scores, k=1)
    return top_results

query = 'ê³ ê¸° ìª½íŒŒ'
top_result = get_query_sim_top_k(query, model, df)


# df.iloc[top_result[1].numpy(), :][['ìš”ë¦¬', 'ì¢…ë¥˜', 'ì¬ë£Œ']]



## ë©”ì„¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
# intentì—ì„œ ì˜ë„ íŒŒì•…í•˜ê³  recom í˜¹ì€ desc ë¡œ íŒë‹¨ë˜ëŠ” ê²ƒ.
msg_prompt = {
    'recom' : {
                'system' : "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•˜ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.", 
                'user' : "ì‚¬ìš©ìì—ê²Œ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•  ë•Œ, 'ğŸ‘¨ğŸ»â€ğŸ³ ê·¸ëŸ¼ìš”!'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê°„ë‹¨í•œ ì¸ì‚¬ë§ 1ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.", 
              },
    'desc' : {
                'system' : "You are a helpful assistant who kindly answers.", 
                'user' : "Please write a simple greeting starting with 'of course' to explain the recipes to the user.", 
              },
    'intent' : {
                'system' : "You are a helpful assistant who understands the intent of the user's question.",
                'user' : "Which category does the sentence below belong to: 'description', 'recommended', 'search'? Show only categories. \n context:"
                }
}

user_msg_history = []



## OpenAI APIì™€ GPT-3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ msgì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
# ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ëŒ€í™” ìƒì„±.
def get_chatgpt_msg(msg):
    completion = openai.ChatCompletion.create(
                    model='gpt-3.5-turbo',
                    messages=msg
                    )
    return completion['choices'][0]['message']['content']



## intentì™€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ prompt ìƒì„±
# ì ì ˆí•œ ì´ˆê¸° ë©”ì„¸ì§€ ìƒì„±, ì‚¬ìš©ìì™€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”êµ¬ì„± ê°€ëŠ¥.
def set_prompt(intent, query, msg_prompt_init, model):
    '''prompt í˜•íƒœë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜'''
    m = dict()
    # ê²€ìƒ‰ ë˜ëŠ” ì¶”ì²œì´ë©´
    if ('recom' in intent) or ('search' in intent):
        msg = msg_prompt_init['recom'] # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ë¥¼ ê°€ì§€ê³ ì˜¤ê³ 
    # ì„¤ëª…ë¬¸ì´ë©´
    elif 'desc' in intent:
        msg = msg_prompt_init['desc'] # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ë¥¼ ê°€ì§€ê³ ì˜¤ê³ 
    # intent íŒŒì•…
    else:
        msg = msg_prompt_init['intent']
        msg['user'] += f' {query} \n A:'
    for k, v in msg.items():
        m['role'], m['content'] = k, v
    return [m]



## ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ gpt ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±.
# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³ , ìƒì„±ëœ ì‘ë‹µì„ ë””ì½”ë”© í•˜ì—¬ ë°˜í™˜.
# ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œë§Œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±(ì´ì „ ëŒ€í™” ê³ ë ¤ x)
def generate_answer(model, tokenizer, input_text, max_len=50):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    # generate text until the specified length
    output = model.generate(input_ids=input_ids, max_length=max_len, do_sample=True, top_p=0.92, top_k=50)
    # decode the generated text
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response


import webbrowser

def user_interact(query, model, msg_prompt_init):
    # 1. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…
    user_intent = set_prompt('intent', query, msg_prompt_init, None)
    user_intent = get_chatgpt_msg(user_intent).lower()
    print("user_intent : ", user_intent)
    
    # 2. ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì— ë”°ë¼ prompt ìƒì„±    
    intent_data = set_prompt(user_intent, query, msg_prompt_init, model)
    intent_data_msg = get_chatgpt_msg(intent_data).replace("\n", "").strip()
    print("intent_data_msg : ", intent_data_msg)
    
# 3-1. ì¶”ì²œ ë˜ëŠ” ê²€ìƒ‰ì´ë©´
    if ('recom' in user_intent) or ('search' in user_intent):
        recom_msg = str()
        # ê¸°ì¡´ì— ë©”ì„¸ì§€ê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
        if (len(user_msg_history) > 0 ) and (user_msg_history[-1]['role'] == 'assistant'):
            query = user_msg_history[-1]['content']['feature']
        # ìœ ì‚¬ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        #top_result = get_query_sim_top_k(query, model, movies_metadata, top_k=1 if 'recom' in user_intent else 3) # ì¶”ì²œ ê°œìˆ˜ ì„¤ì •í•˜ë ¤ë©´!
        top_result = get_query_sim_top_k(query, model, df)
        # ê²€ìƒ‰ì´ë©´, ìê¸° ìì‹ ì˜ ì»¨í…ì¸ ëŠ” ì œì™¸
        top_index = top_result[1].numpy() if 'recom' in user_intent else top_result[1].numpy()[1:]
        # ì¥ë¥´, ì œëª©, overviewë¥¼ ê°€ì ¸ì™€ì„œ ì¶œë ¥
        r_set_d = df.iloc[top_index, :][['ìš”ë¦¬', 'ì„¤ëª…', 'ì¬ë£Œ', 'ì‚¬ì§„', 'ìš”ë¦¬ë°©ë²•']]
        r_set_d = json.loads(r_set_d.to_json(orient="records"))
        for r in r_set_d:
            message(f" {intent_data_msg} \n{str(recom_msg)}")
            st.write("### ğŸ³ì¶”ì²œë©”ë‰´: ã€ " + r['ìš”ë¦¬'] + " ã€‘")
            # 'ì‚¬ì§„' ì»¬ëŸ¼ì—ì„œ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
            img_url = r['ì‚¬ì§„']
            response = requests.get(img_url)
            image_bytes = io.BytesIO(response.content)
            image = Image.open(image_bytes)
            st.image(image, width=500)
            

            st.write("\n")

            # 'ì„¤ëª…' ì¶œë ¥
            message('ğŸ‘¨â€ğŸ³ ê°„ë‹¨í•˜ê²Œ "' + r['ìš”ë¦¬'] + '" ì†Œê°œë¥¼ í•´ë“œë¦´ê²Œìš”. \n\n ' + r['ì„¤ëª…'])
            st.write("\n")
            st.write("\n")
                            

            # ì¬ë£Œêµ¬ë§¤ë§í¬ ì¶œë ¥
            message('ì¬ë£Œê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? \n ì•„ë˜ ì¬ë£Œë¥¼ í´ë¦­í•˜ë©´ êµ¬ë§¤ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.')
                # st.markdown("<p style='color:blue; font-style: italic'> (ì¬ë£Œë¥¼ í´ë¦­í•˜ë©´ êµ¬ë§¤ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.)</p>", unsafe_allow_html=True)
            r_ingredients = r['ì¬ë£Œ'].split()
            button_html = ''
            for ing in r_ingredients:
                gs_url = f"https://m.gsfresh.com/shop/search/searchSect.gs?tq={ing}&mseq=S-11209-0301&keyword={ing}"
                button_html += f"""<a href="{gs_url}" target="_blank" style="text-decoration: none; color: white; background-color: #FF5733; padding: 6px 12px; border-radius: 5px; margin-right: 5px;">{ing}</a>"""
            st.markdown(button_html, unsafe_allow_html=True)



            # ë ˆì‹œí”¼ í† ê¸€ë¡œ ì¶œë ¥
            with st.expander('##### ğŸ½ï¸ ìš”ë¦¬ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?'):
                recipe_steps = r['ìš”ë¦¬ë°©ë²•'].replace('[', '').replace(']', '').replace("'", "").replace("\\r\\n", ' ').split(", ")
                for i, step in enumerate(recipe_steps):
                    step = step.strip()  
                    if step:  
                        st.write(f"{i+1}. {step}")



            
        user_msg_history.append({'role' : 'assistant', 'content' : f"{intent_data_msg} {str(recom_msg)}"})
       






        
    # 3-2. ì„¤ëª…ì´ë©´
    elif 'desc' in user_intent:
        try:
            # ì´ì „ ë©”ì„¸ì§€ì— ë”°ë¼ì„œ ì„¤ëª…ì„ ê°€ì ¸ì™€ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì´ì „ ë©”ì„¸ì§€ ì»¨í…ì¸ ë¥¼ ê°€ì ¸ì˜´
            prev_msg = next(filter(lambda x: x['role'] == 'user', reversed(user_msg_history)))
            top_result = get_query_sim_top_k(query, model, df)
            if len(top_result[1]) == 0:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°, ìœ ì‚¬í•œ ê²°ê³¼ ì¤‘ì—ì„œ ê°€ì¥ ìƒìœ„ 1ê°œë¥¼ ê°€ì ¸ì˜´
                top_result = get_query_sim_top_k(query, model, df, k=1)
            # featureê°€ ìƒì„¸ ì„¤ëª…ì´ë¼ê³  ê°€ì •í•˜ê³  í•´ë‹¹ ì»¬ëŸ¼ì˜ ê°’ì„ ê°€ì ¸ì˜´
            desc = top_result[0]['overview']
            user_msg_history.append({'role' : 'assistant', 'content' : f"{intent_data_msg} {desc}"})
            return (f"\ndesc data : {intent_data_msg} \n{desc}\n")
        except (StopIteration, IndexError):
            user_msg_history.append({'role' : 'assistant', 'content' : "ì´ì „ì— ìš”ì²­ëœ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤."})

    # 3-3. ì¶”ì²œ ë˜ëŠ” ê²€ìƒ‰ì´ ì•„ë‹ˆë©´
    else:
        # ìƒˆë¡œìš´ ì¿¼ë¦¬ì™€ ê¸°ì¡´ ëŒ€í™”ë¥¼ ëª¨ë‘ ì´ìš©í•´ ìƒì„±ëœ promptì—ì„œ ë‹µë³€ ìƒì„±
        answer = generate_answer(query, user_msg_history, model)
        user_msg_history.append({'role' : 'assistant', 'content' : answer})
        return answer



# query = "ë¼ì§€ê³ ê¸°ì™€ ì–‘íŒŒê°€ ë“¤ì–´ê°„ ìŒì‹ì„ ì¶”ì²œí•´ì¤˜"
# user_interact(query, model, copy.deepcopy(msg_prompt))




# def generate_response(prompt, df=None, past=None):
#     if df is not None:
#         # ë ˆì‹œí”¼ ë°ì´í„°ë¥¼ ì´ìš©í•œ prompt ìƒì„±
#         prompt = f"{prompt}\nRecipes:"
#         for i, row in df.iterrows():
#             prompt = f"{prompt}\n{i+1}. {row['title']}"
#     if past is not None:
#         # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì´ìš©í•œ prompt ìƒì„±
#         prompt = f"{prompt}\nPast messages:"
#         for msg in past:
#             prompt = f"{prompt}\n- {msg}"
    
#     completions = openai.Completion.create (
#         engine="text-davinci-003",
#         prompt=prompt,
#         max_tokens=1024,
#         stop=None,
#         temperature=0,
#         top_p=1,
#     )
#     message = completions["choices"][0]["text"].replace("\n", "")
#     return message







if __name__ == "__main__":
    st.title('ğŸ¤– ë ˆì‹œí”¼ ì¶”ì²œ ì±—ë´‡, [ë ˆì±—!]')
    st.write("""
        ê°€ì§€ê³  ìˆëŠ” ì¬ë£Œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ê¶ê¸ˆí•œ ë ˆì‹œí”¼ë¥¼ "ğŸ¤–ë ˆì³‡"ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!
        """)
    st.write('\n')
    
    # ì±—ë´‡ ìƒì„±í•˜ê¸°
    if not hasattr(st.session_state, 'generated'):
        st.session_state.generated = []

    if not hasattr(st.session_state, 'past'):
        st.session_state.past = []

    


    # ì¿¼ë¦¬ ë³€í˜•í•˜ê¸°
    query = None
    with st.form(key='my_form'):
        query = st.text_input(f"Your Query:")
        submitted = st.form_submit_button('Send')



    # ìˆ˜í–‰ë¬¸ ë§Œë“¤ê¸°
    if submitted and query:
        output = user_interact(query, model, msg_prompt)
        st.session_state.past.append(query)
        st.session_state.generated.append(output)


        
    # ì¶œë ¥í•˜ê¸°
    if st.session_state['generated']:
        for i in range(len(st.session_state['generated'])-1, -1, -1):
            message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')

