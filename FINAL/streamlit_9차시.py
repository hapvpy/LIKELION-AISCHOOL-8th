# ì‚¬ì§„ ì˜† ì¬ë£Œë²„íŠ¼ -> ë§í¬ì—°ê²° (ì±—ë´‡í˜•ì‹0)
# ë¯¼ê·œë‹˜ íŒŒì¼ ì—°ê²°


# ë°ì´í„° ë¶„ì„
import pandas as pd
import numpy as np

# ì§„í–‰ì‹œê°„ í‘œì‹œ
import swifter
from tqdm.notebook import tqdm

## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st 

# íŒŒì´í† ì¹˜
import torch

# ë¬¸ì¥ ì„ë² ë”©, transformer ìœ í‹¸ë¦¬í‹°
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer, util
from sentence_transformers import SentenceTransformer, models
# Owl-ViTë¥¼ ìœ„í•œ ì „ì²˜ë¦¬, ê°ì²´ ê°ì§€
from transformers import OwlViTProcessor, OwlViTForObjectDetection
# íŒŒì´í”„ë¼ì¸ êµ¬ì„±
from transformers import pipeline
# GPT-2 í† í¬ë‚˜ì´ì €
from transformers import GPT2TokenizerFast

# ì´ë¯¸ì§€ ì²˜ë¦¬
from PIL import Image
# ì‚¬ì´í‚· ëŸ°
import sklearn.datasets as datasets
import sklearn.manifold as manifold

# ë°ì´í„° ìˆ˜ì§‘
import requests
from bs4 import BeautifulSoup

# ê°ì²´ ë³µì‚¬
import copy
# JSON í˜•ì‹ ë°ì´í„° ì²˜ë¦¬
import json
# íƒ€ì… íŒíŠ¸
from typing import List, Tuple, Dict

# ë°ì´í„°ë² ì´ìŠ¤ í™œìš©
import sqlite3 
import pickle

# OpenAI API í™œìš©
import openai 
import os # ìš´ì˜ì²´ì œ
import sys # íŒŒì´ì¬ ë³€ìˆ˜, í•¨ìˆ˜ ì—‘ì„¸ìŠ¤ 
from dotenv import load_dotenv # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ(API Key ë³´ì•ˆ)
import io

# ìŠ¤íŠ¸ë¦¼ë¦¿ êµ¬í˜„
import streamlit
from streamlit_chat import message



## íŒŒì¼ ë° API ê°€ì ¸ì˜¤ê¸°
# app.py íŒŒì¼ì´ ìœ„ì¹˜í•œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
APP_DIR = os.path.dirname(os.path.abspath(__file__))
# app.pyì—ì„œ ë§Œë“  ë°ì´í„°í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°
LAST_DF_PATH = os.path.join(APP_DIR, '..', 'last_df.pkl')
df = pd.read_pickle(LAST_DF_PATH)
# load_dotenv()    
# openai.api_key = os.getenv('OPENAI_API_KEY')
openai.api_key = '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'

# ì‹¤í–‰ os í™•ì¸
cur_os = sys.platform



# íŒŒìƒ ë³€ìˆ˜
# - feature1 = 'ì¬ë£Œ'
# - feature2 = 'ì¬ë£Œ' + 'ìš”ë¦¬'
# - feature3 = 'ì¬ë£Œ' + 'ìš”ë¦¬' + 'ì¢…ë¥˜'
# - feature4 = 'ì¬ë£Œ' + 'ìš”ë¦¬' + 'ì¢…ë¥˜' + 'ë‚œì´ë„'
# - feature5 = 'ì¬ë£Œ' + 'ìš”ë¦¬' + 'ì¢…ë¥˜' + 'ë‚œì´ë„' + 'ìš”ë¦¬ë°©ë²•'
# - feature6 = 'ì¬ë£Œ' + 'ìš”ë¦¬' + 'ì„¤ëª…' + 'ì¢…ë¥˜' + 'ë‚œì´ë„' + 'ìš”ë¦¬ë°©ë²•'



## ëª¨ë¸ ì„ ì–¸
model_name = 'jhgan/ko-sroberta-multitask'
model = SentenceTransformer(model_name)



## ìƒìœ„ 5ê°œ í•­ëª© ì¶œë ¥(ë§í¬ë¡œ ì¤‘ë³µì œê±°-ë¯¼ê·œë‹˜)
def get_query_sim_top_k(query, model, df):
    "ì¿¼ë¦¬ì™€ ë°ì´í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê³  ìœ ì‚¬í•œ ìˆœìœ„ 5ê°œ ë°˜í™˜"
    query_encode = model.encode(query)
    cos_scores = util.pytorch_cos_sim(query_encode, df['ko-sroberta-multitask-feature6'])[0]
    top_results = torch.topk(cos_scores, k=1)
    return top_results

query = 'ê³ ê¸° ìª½íŒŒ'
top_result = get_query_sim_top_k(query, model, df)


# df.iloc[top_result[1].numpy(), :][['ìš”ë¦¬', 'ì¢…ë¥˜', 'ì¬ë£Œ']]



## ë©”ì„¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
# intentì—ì„œ ì˜ë„ íŒŒì•…í•˜ê³  recom í˜¹ì€ desc ë¡œ íŒë‹¨ë˜ëŠ” ê²ƒ.
msg_prompt = {
    'recom' : {
                'system' : "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•˜ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.", 
                'user' : "ì‚¬ìš©ìì—ê²Œ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•  ë•Œ, 'ğŸ‘¨ğŸ»â€ğŸ³ ê·¸ëŸ¼ìš”!'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê°„ë‹¨í•œ ì¸ì‚¬ë§ 1ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.", 
              },
    'desc' : {
                'system' : "You are a helpful assistant who kindly answers.", 
                'user' : "Please write a simple greeting starting with 'of course' to explain the recipes to the user.", 
              },
    'intent' : {
                'system' : "You are a helpful assistant who understands the intent of the user's question.",
                'user' : "Which category does the sentence below belong to: 'description', 'recommended', 'search'? Show only categories. \n context:"
                }
}

user_msg_history = []



## OpenAI APIì™€ GPT-3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ msgì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
# ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ëŒ€í™” ìƒì„±.
def get_chatgpt_msg(msg):
    completion = openai.ChatCompletion.create(
                    model='gpt-3.5-turbo',
                    messages=msg
                    )
    return completion['choices'][0]['message']['content']



## intentì™€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ prompt ìƒì„±
# ì ì ˆí•œ ì´ˆê¸° ë©”ì„¸ì§€ ìƒì„±, ì‚¬ìš©ìì™€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”êµ¬ì„± ê°€ëŠ¥.
def set_prompt(intent, query, msg_prompt_init, model):
    '''prompt í˜•íƒœë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜'''
    m = dict()
    # ê²€ìƒ‰ ë˜ëŠ” ì¶”ì²œì´ë©´
    if ('recom' in intent) or ('search' in intent):
        msg = msg_prompt_init['recom'] # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ë¥¼ ê°€ì§€ê³ ì˜¤ê³ 
    # ì„¤ëª…ë¬¸ì´ë©´
    elif 'desc' in intent:
        msg = msg_prompt_init['desc'] # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ë¥¼ ê°€ì§€ê³ ì˜¤ê³ 
    # intent íŒŒì•…
    else:
        msg = msg_prompt_init['intent']
        msg['user'] += f' {query} \n A:'
    for k, v in msg.items():
        m['role'], m['content'] = k, v
    return [m]



## ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ gpt ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±.
# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³ , ìƒì„±ëœ ì‘ë‹µì„ ë””ì½”ë”© í•˜ì—¬ ë°˜í™˜.
# ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œë§Œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±(ì´ì „ ëŒ€í™” ê³ ë ¤ x)
def generate_answer(model, tokenizer, input_text, max_len=50):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    # generate text until the specified length
    output = model.generate(input_ids=input_ids, max_length=max_len, do_sample=True, top_p=0.92, top_k=50)
    # decode the generated text
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response


import webbrowser

def user_interact(query, model, msg_prompt_init):
    # 1. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…
    user_intent = set_prompt('intent', query, msg_prompt_init, None)
    user_intent = get_chatgpt_msg(user_intent).lower()
    print("user_intent : ", user_intent)
    
    # 2. ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì— ë”°ë¼ prompt ìƒì„±    
    intent_data = set_prompt(user_intent, query, msg_prompt_init, model)
    intent_data_msg = get_chatgpt_msg(intent_data).replace("\n", "").strip()
    print("intent_data_msg : ", intent_data_msg)
    
# 3-1. ì¶”ì²œ ë˜ëŠ” ê²€ìƒ‰ì´ë©´
    if ('recom' in user_intent) or ('search' in user_intent):
        recom_msg = str()
        # ê¸°ì¡´ì— ë©”ì„¸ì§€ê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
        if (len(user_msg_history) > 0 ) and (user_msg_history[-1]['role'] == 'assistant'):
            query = user_msg_history[-1]['content']['feature']
        # ìœ ì‚¬ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        #top_result = get_query_sim_top_k(query, model, movies_metadata, top_k=1 if 'recom' in user_intent else 3) # ì¶”ì²œ ê°œìˆ˜ ì„¤ì •í•˜ë ¤ë©´!
        top_result = get_query_sim_top_k(query, model, df)
        # ê²€ìƒ‰ì´ë©´, ìê¸° ìì‹ ì˜ ì»¨í…ì¸ ëŠ” ì œì™¸
        top_index = top_result[1].numpy() if 'recom' in user_intent else top_result[1].numpy()[1:]
        # ì¥ë¥´, ì œëª©, overviewë¥¼ ê°€ì ¸ì™€ì„œ ì¶œë ¥
        r_set_d = df.iloc[top_index, :][['ìš”ë¦¬', 'ì„¤ëª…', 'ì¬ë£Œ', 'ì‚¬ì§„', 'ìš”ë¦¬ë°©ë²•']]
        r_set_d = json.loads(r_set_d.to_json(orient="records"))
        for r in r_set_d:
            message(f" {intent_data_msg} \n{str(recom_msg)}")
            st.write("### ğŸ³ì¶”ì²œë©”ë‰´: ã€ " + r['ìš”ë¦¬'] + " ã€‘")
            # 'ì‚¬ì§„' ì»¬ëŸ¼ì—ì„œ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
            img_url = r['ì‚¬ì§„']
            response = requests.get(img_url)
            image_bytes = io.BytesIO(response.content)
            image = Image.open(image_bytes)
            st.image(image, width=500)
            

            st.write("\n")

            # 'ì„¤ëª…' ì¶œë ¥
            message('ğŸ‘¨â€ğŸ³ ê°„ë‹¨í•˜ê²Œ "' + r['ìš”ë¦¬'] + '" ì†Œê°œë¥¼ í•´ë“œë¦´ê²Œìš”. \n\n ' + r['ì„¤ëª…'])
            st.write("\n")
            st.write("\n")
                            

            # ì¬ë£Œêµ¬ë§¤ë§í¬ ì¶œë ¥
            message('ì¬ë£Œê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? \n ì•„ë˜ ì¬ë£Œë¥¼ í´ë¦­í•˜ë©´ êµ¬ë§¤ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.')
                # st.markdown("<p style='color:blue; font-style: italic'> (ì¬ë£Œë¥¼ í´ë¦­í•˜ë©´ êµ¬ë§¤ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.)</p>", unsafe_allow_html=True)
            r_ingredients = r['ì¬ë£Œ'].split()
            button_html = ''
            for ing in r_ingredients:
                gs_url = f"https://m.gsfresh.com/shop/search/searchSect.gs?tq={ing}&mseq=S-11209-0301&keyword={ing}"
                button_html += f"""<a href="{gs_url}" target="_blank" style="text-decoration: none; color: white; background-color: #FF5733; padding: 6px 12px; border-radius: 5px; margin-right: 5px;">{ing}</a>"""
            st.markdown(button_html, unsafe_allow_html=True)



            # ë ˆì‹œí”¼ í† ê¸€ë¡œ ì¶œë ¥
            with st.expander('##### ğŸ½ï¸ ìš”ë¦¬ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?'):
                recipe_steps = r['ìš”ë¦¬ë°©ë²•'].replace('[', '').replace(']', '').replace("'", "").replace("\\r\\n", ' ').split(", ")
                for i, step in enumerate(recipe_steps):
                    step = step.strip()  
                    if step:  
                        st.write(f"{i+1}. {step}")

        user_msg_history.append({'role' : 'assistant', 'content' : f"{intent_data_msg} {str(recom_msg)}"})
       

        
    # 3-2. ì„¤ëª…ì´ë©´
    elif 'desc' in user_intent:
        try:
            # ì´ì „ ë©”ì„¸ì§€ì— ë”°ë¼ì„œ ì„¤ëª…ì„ ê°€ì ¸ì™€ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì´ì „ ë©”ì„¸ì§€ ì»¨í…ì¸ ë¥¼ ê°€ì ¸ì˜´
            prev_msg = next(filter(lambda x: x['role'] == 'user', reversed(user_msg_history)))
            top_result = get_query_sim_top_k(query, model, df)
            if len(top_result[1]) == 0:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°, ìœ ì‚¬í•œ ê²°ê³¼ ì¤‘ì—ì„œ ê°€ì¥ ìƒìœ„ 1ê°œë¥¼ ê°€ì ¸ì˜´
                top_result = get_query_sim_top_k(query, model, df, k=1)
            # featureê°€ ìƒì„¸ ì„¤ëª…ì´ë¼ê³  ê°€ì •í•˜ê³  í•´ë‹¹ ì»¬ëŸ¼ì˜ ê°’ì„ ê°€ì ¸ì˜´
            desc = top_result[0]['overview']
            user_msg_history.append({'role' : 'assistant', 'content' : f"{intent_data_msg} {desc}"})
            return (f"\ndesc data : {intent_data_msg} \n{desc}\n")
        except (StopIteration, IndexError):
            user_msg_history.append({'role' : 'assistant', 'content' : "ì´ì „ì— ìš”ì²­ëœ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤."})

    # 3-3. ì¶”ì²œ ë˜ëŠ” ê²€ìƒ‰ì´ ì•„ë‹ˆë©´
    else:
        # ìƒˆë¡œìš´ ì¿¼ë¦¬ì™€ ê¸°ì¡´ ëŒ€í™”ë¥¼ ëª¨ë‘ ì´ìš©í•´ ìƒì„±ëœ promptì—ì„œ ë‹µë³€ ìƒì„±
        answer = generate_answer(query, user_msg_history, model)
        user_msg_history.append({'role' : 'assistant', 'content' : answer})
        return answer




if __name__ == "__main__":
    st.title('ğŸ¤– ë ˆì‹œí”¼ ì¶”ì²œ ì±—ë´‡, [ë ˆì±—!]')
    st.write("""
        ê°€ì§€ê³  ìˆëŠ” ì¬ë£Œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ê¶ê¸ˆí•œ ë ˆì‹œí”¼ë¥¼ "ğŸ¤–ë ˆì³‡"ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!
        """)
    st.write('\n')
    
    # ì±—ë´‡ ìƒì„±í•˜ê¸°
    if not hasattr(st.session_state, 'generated'):
        st.session_state.generated = []

    if not hasattr(st.session_state, 'past'):
        st.session_state.past = []

    


    # ì¿¼ë¦¬ ë³€í˜•í•˜ê¸°
    query = None
    with st.form(key='my_form'):
        query = st.text_input(f"Your Query:")
        submitted = st.form_submit_button('Send')



    # ìˆ˜í–‰ë¬¸ ë§Œë“¤ê¸°
    if submitted and query:
        output = user_interact(query, model, msg_prompt)
        st.session_state.past.append(query)
        st.session_state.generated.append(output)


        
    # ì¶œë ¥í•˜ê¸°
    if st.session_state['generated']:
        for i in range(len(st.session_state['generated'])-1, -1, -1):
            message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')