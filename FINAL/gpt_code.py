# -*- coding: utf-8 -*-
"""gpt_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lrmeg3eFkqkbE_Rx9yb3NDW7tgBzIh2l
"""

import requests

import pandas as pd
import numpy as np
import copy
import json

from ast import literal_eval

import torch
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModel
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from transformers import pipeline
from transformers import GPT2TokenizerFast
from PIL import Image

from tqdm import tqdm
tqdm.pandas()

import pickle

import matplotlib.pyplot as plt
from typing import List, Tuple, Dict

import sklearn.datasets as datasets
import sklearn.manifold as manifold

import openai
import os
import sys
from dotenv import load_dotenv

load_dotenv()    
openai.api_key = os.getenv('OPENAI_API_KEY')

cur_os = sys.platform

# mps_device = torch.device('mps')

df = pd.read_csv('../data/recipes_after.csv')
df.shape

df.head(2)

df['feature'] = df['ì¬ë£Œ']
df.head(2)

model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
model

df['embeddings'] = df['feature'].progress_apply(lambda x : model.encode(x))
df.shape

df.head(2)

def get_query_sim_top_k(query, model, df):
    query_encode = model.encode(query)
    cos_scores = util.pytorch_cos_sim(query_encode, df['embeddings'])[0]
    top_results = torch.topk(cos_scores, k=5)
    return top_results

query = 'ê³ ê¸° ìª½íŒŒ'
top_result = get_query_sim_top_k(query, model, df)

top_result

df.iloc[top_result[1].numpy(), :][['ìš”ë¦¬', 'ì¢…ë¥˜', 'ì¬ë£Œ']]

def print_msg(msg):
    completion = openai.ChatCompletion.create(
                    model='gpt-3.5-turbo',
                    messages=msg
                    )
    return completion['choices'][0]['message']['content']

msg_prompt = {
    'recom' : {
                'system' : "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•˜ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.", 
                'user' : "ì‚¬ìš©ìì—ê²Œ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•  ë•Œ, 'ğŸ‘¨ğŸ»â€ğŸ³ ê·¸ëŸ¼ìš”!'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê°„ë‹¨í•œ ì¸ì‚¬ë§ 1ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.", 
              },
    'desc' : {
                'system' : "You are a helpful assistant who kindly answers.", 
                'user' : "Please write a simple greeting starting with 'of course' to explain the recipes to the user.", 
              },
    'intent' : {
                'system' : "You are a helpful assistant who understands the intent of the user's question.",
                'user' : "Which category does the sentence below belong to: 'description', 'recommended', 'search'? Show only categories. \n context:"
                }
}

msg_prompt

user_msg_history = []

def get_chatgpt_msg(msg):
    completion = openai.ChatCompletion.create(
                    model='gpt-3.5-turbo',
                    messages=msg
                    )
    return completion['choices'][0]['message']['content']

def set_prompt(intent, query, msg_prompt_init, model):
    '''prompt í˜•íƒœë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜'''
    m = dict()
    # ê²€ìƒ‰ ë˜ëŠ” ì¶”ì²œì´ë©´
    if ('recom' in intent) or ('search' in intent):
        msg = msg_prompt_init['recom'] # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ë¥¼ ê°€ì§€ê³ ì˜¤ê³ 
    # ì„¤ëª…ë¬¸ì´ë©´
    elif 'desc' in intent:
        msg = msg_prompt_init['desc'] # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ë¥¼ ê°€ì§€ê³ ì˜¤ê³ 
    # intent íŒŒì•…
    else:
        msg = msg_prompt_init['intent']
        msg['user'] += f' {query} \n A:'
    for k, v in msg.items():
        m['role'], m['content'] = k, v
    return [m]

def user_interact(query, model, msg_prompt_init):
    # 1. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…
    user_intent = set_prompt('intent', query, msg_prompt_init, None)
    user_intent = get_chatgpt_msg(user_intent).lower()
    print("user_intent : ", user_intent)
    
    # 2. ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì— ë”°ë¼ prompt ìƒì„±    
    intent_data = set_prompt(user_intent, query, msg_prompt_init, model)
    intent_data_msg = get_chatgpt_msg(intent_data).replace("\n", "").strip()
    print("intent_data_msg : ", intent_data_msg)
    
    # 3-1. ì¶”ì²œ ë˜ëŠ” ê²€ìƒ‰ì´ë©´
    if ('recom' in user_intent) or ('search' in user_intent):
        recom_msg = str()
        # ê¸°ì¡´ì— ë©”ì„¸ì§€ê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
        if (len(user_msg_history) > 0 ) and (user_msg_history[-1]['role'] == 'assistant'):
            query = user_msg_history[-1]['content']['feature']
        # ìœ ì‚¬ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        #top_result = get_query_sim_top_k(query, model, movies_metadata, top_k=1 if 'recom' in user_intent else 3) # ì¶”ì²œ ê°œìˆ˜ ì„¤ì •í•˜ë ¤ë©´!
        top_result = get_query_sim_top_k(query, model, df)
        #print("top_result : ", top_result)
        # ê²€ìƒ‰ì´ë©´, ìê¸° ìì‹ ì˜ ì»¨í…ì¸ ëŠ” ì œì™¸
        top_index = top_result[1].numpy() if 'recom' in user_intent else top_result[1].numpy()[1:]
        #print("top_index : ", top_index)
        # ì¥ë¥´, ì œëª©, overviewë¥¼ ê°€ì ¸ì™€ì„œ ì¶œë ¥
        r_set_d = df.iloc[top_index, :][['ìš”ë¦¬', 'ì¢…ë¥˜', 'ì¬ë£Œ']]
        r_set_d = json.loads(r_set_d.to_json(orient="records"))
        for r in r_set_d:
            for _, v in r.items():
                recom_msg += f"{v} \n"
            recom_msg += "\n"
        user_msg_history.append({'role' : 'assistant', 'content' : f"{intent_data_msg} {str(recom_msg)}"})
        print(f"\nrecom data : {intent_data_msg} \n{str(recom_msg)}")
    # 3-2. ì„¤ëª…ì´ë©´
    elif 'desc' in user_intent:
        # ì´ì „ ë©”ì„¸ì§€ì— ë”°ë¼ì„œ ì„¤ëª…ì„ ê°€ì ¸ì™€ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì´ì „ ë©”ì„¸ì§€ ì»¨í…ì¸ ë¥¼ ê°€ì ¸ì˜´
        top_result = get_query_sim_top_k(user_msg_history[-1]['content'], model, df)
        # featureê°€ ìƒì„¸ ì„¤ëª…ì´ë¼ê³  ê°€ì •í•˜ê³  í•´ë‹¹ ì»¬ëŸ¼ì˜ ê°’ì„ ê°€ì ¸ì™€ ì¶œë ¥
        r_set_d = df.iloc[top_result[1].numpy(), :][['feature']]
        r_set_d = json.loads(r_set_d.to_json(orient="records"))[0]
        user_msg_history.append({'role' : 'assistant', 'content' : r_set_d})
        print(f"\n describe : {intent_data_msg} {r_set_d}")

query = "ë¼ì§€ê³ ê¸°ì™€ ì–‘íŒŒê°€ ë“¤ì–´ê°„ ìŒì‹ì„ ì¶”ì²œí•´ì¤˜"
user_interact(query, model, copy.deepcopy(msg_prompt))